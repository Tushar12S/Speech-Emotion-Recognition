
Speech Emotion Recognition System

This project showcases an advanced Speech Emotion Recognition system that utilizes the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset to accurately detect and classify emotions in speech. By employing a Multi-Layer Perceptron (MLP) neural network, this system analyzes various audio features to distinguish key emotional states. It effectively identifies and categorizes emotions from spoken audio, leveraging a state-of-the-art MLP architecture for robust performance. Additionally, the system includes visual tools to interpret prediction results clearly.

Overview Dataset: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) 
Technologies: Python, Google Colab, librosa, soundfile, numpy, pandas, scikit-learn 
Getting Started Google Colab: Open a new notebook and mount Google Drive to access the RAVDESS dataset. Install Dependencies: Use pip to install the required libraries. 
Feature Extraction: Extract audio features (MFCC, Chroma, Mel spectrogram). 
Model Training: Train the MLP model on the extracted features. 
Evaluation: Assess model performance using accuracy and F1 score.

Contributions to this project are encouraged! Feel free to fork the repository, submit pull requests, and help enhance the capabilities of this Speech Emotion Recognition system. 
